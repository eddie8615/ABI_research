{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "59ae74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaa248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = 25253\n",
    "# validation_len = 9471\n",
    "# test_len = 13794\n",
    "train_length = 2684\n",
    "val_length = 726\n",
    "test_length = 1498\n",
    "\n",
    "def load_features(filename,\n",
    "                  skip_header=True,\n",
    "                  skip_instname=True,\n",
    "                  delim=' ',\n",
    "                  num_lines=0):\n",
    "    if num_lines == 0:\n",
    "        num_lines = get_num_lines(filename, skip_header)\n",
    "\n",
    "    data = np.empty(\n",
    "        (num_lines, 50), float)\n",
    "\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        if skip_header:\n",
    "            next(csv_file)\n",
    "        c = 0\n",
    "        for line in tqdm(csv_file):\n",
    "            offset = 0\n",
    "            if skip_instname:\n",
    "                offset = line.find(delim) + 1\n",
    "            data[c, :] = np.fromstring(line[offset:], dtype=float, sep=delim)\n",
    "            c += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_batch_features(filename, start_index=0, amount=0):\n",
    "    delim = ' '\n",
    "    \n",
    "    data = np.empty((amount, 25), float)\n",
    "    \n",
    "    with open(filename, 'r') as csv_file:\n",
    "        for i, line in tqdm(enumerate(csv_file)):\n",
    "            if i < start_index:\n",
    "                continue\n",
    "            if i >= start_index + amount:\n",
    "                break\n",
    "            index = i - start_index\n",
    "            data[index, :] = np.fromstring(line, dtype=float, sep=delim)\n",
    "    return data\n",
    "    \n",
    "def load_batch_labels(filename, start_index=1, amount=0):\n",
    "    labels = np.empty((amount, 3), float)\n",
    "    delim = ','\n",
    "    \n",
    "    with open(filename, 'r') as csv_file:\n",
    "        for i, line in tqdm(enumerate(csv_file)):\n",
    "            if i < start_index:\n",
    "                continue\n",
    "            if i >= start_index + amount:\n",
    "                break\n",
    "            cols = np.fromstring(line, dtype=float, sep=delim)\n",
    "            index = i - start_index\n",
    "            labels[index, :] = cols[1:]\n",
    "    return labels\n",
    "    \n",
    "def get_num_lines(filename, skip_header):\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        if skip_header:\n",
    "            next(csv_file)\n",
    "        c = 0\n",
    "        for line in csv_file:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def load_labels(filename,\n",
    "                  skip_header=True,\n",
    "                  skip_instname=True,\n",
    "                  delim=' ',\n",
    "                  num_lines=0):\n",
    "    if num_lines == 0:\n",
    "        num_lines = get_num_lines(filename, skip_header)\n",
    "\n",
    "    data = np.empty(\n",
    "        (num_lines, 3), float)\n",
    "\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        if skip_header:\n",
    "            next(csv_file)\n",
    "        c = 0\n",
    "        for line in tqdm(csv_file):\n",
    "            offset = 0\n",
    "            if skip_instname:\n",
    "                offset = line.find(delim) + 1\n",
    "            data[c, :] = np.fromstring(line[offset:], dtype=float, sep=delim)\n",
    "            c += 1\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def get_scaler(x):\n",
    "    x_scaler = StandardScaler()\n",
    "    x_scaler.fit(x)\n",
    "  \n",
    "    return x_scaler\n",
    "\n",
    "def scale_data(scaler, data):\n",
    "    if data.ndim > 2:\n",
    "        data = data.reshape(-1, data.shape[2])\n",
    "    scaled = scaler.transform(data)\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "def ccc(gold, pred):\n",
    "    gold       = K.squeeze(gold, axis=-1)\n",
    "    pred       = K.squeeze(pred, axis=-1)\n",
    "    gold_mean  = K.mean(gold, axis=-1, keepdims=True)\n",
    "    pred_mean  = K.mean(pred, axis=-1, keepdims=True)\n",
    "    covariance = (gold-gold_mean)*(pred-pred_mean)\n",
    "    gold_var   = K.mean(K.square(gold-gold_mean), axis=-1,  keepdims=True)\n",
    "    pred_var   = K.mean(K.square(pred-pred_mean), axis=-1, keepdims=True)\n",
    "    ccc        = K.constant(2.) * covariance / (gold_var + pred_var + K.square(gold_mean - pred_mean) + K.epsilon())\n",
    "    return ccc\n",
    "\n",
    "def ccc_loss(gold, pred):\n",
    "    ccc_loss = K.constant(1.) - ccc(gold, pred)\n",
    "    return ccc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d39c15",
   "metadata": {},
   "source": [
    "### Batch loading to train LSTM-RNN\n",
    "\n",
    "- First, load all data to get scalers that covers for each partition data\n",
    "- Batching the data to train\n",
    "- Batching the data to validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f69b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1041392it [00:07, 135137.16it/s]\n",
      "1041392it [00:01, 577773.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished, Scaling...\n",
      "x_train shape: (2684, 388, 50)\n",
      "y_train shape: (2684, 388, 3)\n",
      "End of loading and preprocessing training samples\n",
      "Loading validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281688it [00:02, 126534.20it/s]\n",
      "281688it [00:00, 478791.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished, Scaling...\n",
      "x_validation shape: (726, 388, 50)\n",
      "y_validation shape: (726, 388, 3)\n",
      "End of loading and preprocessing validation samples\n",
      "Loading testing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "581224it [00:04, 127540.49it/s]\n",
      "581224it [00:01, 479827.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished, Scaling...\n",
      "x_test shape: (1498, 388, 50)\n",
      "y_test shape: (1498, 388, 3)\n",
      "End of loading and preprocessing test samples\n"
     ]
    }
   ],
   "source": [
    "data_path = './Functional_features/'\n",
    "\n",
    "seq_len = 388\n",
    "n_features = 25\n",
    "\n",
    "\n",
    "# load all data to get a scaler that covers all data\n",
    "print(\"Loading training samples...\")\n",
    "x_train = load_features(data_path+'train.txt', skip_header=False, skip_instname=False)\n",
    "train_labels = load_labels(data_path+'train_labels.txt', skip_header=False, skip_instname=False)\n",
    "y_train = train_labels.reshape((train_length, seq_len, 3))\n",
    "\n",
    "print(\"Loading finished, Scaling...\")\n",
    "x_scaler = get_scaler(x_train)\n",
    "\n",
    "# Scaling acoustic features\n",
    "x_train_scaled = scale_data(x_scaler, x_train)\n",
    "# Scaling labels from [-100,100] to [-1, 1]\n",
    "f = lambda x: x * 0.01\n",
    "y_train_scaled = f(y_train)\n",
    "x_train_scaled = x_train_scaled.reshape((train_length, seq_len, n_features * 2))\n",
    "print('x_train shape:', x_train_scaled.shape)\n",
    "print('y_train shape:', y_train_scaled.shape)\n",
    "print(\"End of loading and preprocessing training samples\")\n",
    "\n",
    "print(\"Loading validation samples...\")\n",
    "x_validation = load_features(data_path+'validation.txt', skip_header = False, skip_instname=False)\n",
    "val_labels = load_labels(data_path+'validation_labels.txt', skip_header=False, skip_instname=False)\n",
    "y_validation = val_labels.reshape((val_length, seq_len, 3))\n",
    "print(\"Loading finished, Scaling...\")\n",
    "x_val_scaled = scale_data(x_scaler, x_validation)\n",
    "y_val_scaled = f(y_validation)\n",
    "x_val_scaled = x_val_scaled.reshape((val_length, seq_len, n_features * 2))\n",
    "\n",
    "print('x_validation shape:', x_val_scaled.shape)\n",
    "print('y_validation shape:', y_val_scaled.shape)\n",
    "print(\"End of loading and preprocessing validation samples\")\n",
    "\n",
    "print(\"Loading testing samples...\")\n",
    "x_test = load_features(data_path+'test.txt', skip_header = False, skip_instname=False)\n",
    "test_labels = load_labels(data_path+'test_labels.txt', skip_header=False, skip_instname=False)\n",
    "y_test = test_labels.reshape((test_length, seq_len, 3))\n",
    "print(\"Loading finished, Scaling...\")\n",
    "x_test_scaled = scale_data(x_scaler, x_test)\n",
    "y_test_scaled = f(y_test)\n",
    "x_test_scaled = x_test_scaled.reshape((test_length, seq_len, n_features * 2))\n",
    "\n",
    "print('x_test shape:', x_test_scaled.shape)\n",
    "print('y_test shape:', y_test_scaled.shape)\n",
    "print(\"End of loading and preprocessing test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a161c73-9701-4ff4-ad61-cc24fc47aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = y_train.columns\n",
    "# print(headers)\n",
    "headers = ['Arousal', 'Valence', 'Dominance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeed576-7055-44b8-b324-d2c887cd50f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2684"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13dc203",
   "metadata": {},
   "source": [
    "### Building RNN-LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3bb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model, save_model, load_model, Sequential\n",
    "from keras.layers import Input, Dense, Masking, LSTM, Dropout, TimeDistributed, Bidirectional, Flatten, Embedding, Conv1D, BatchNormalization, MaxPool1D\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1e34a-531f-484a-a58e-8fe1b733ba38",
   "metadata": {},
   "source": [
    "### Multi-task learning\n",
    "\n",
    "three models are integrated and each model covers Arousal, Valence, Dominance respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40323eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_length = 25253\n",
    "# val_length = 9471\n",
    "# test_length = 13794\n",
    "# train_length = 41815\n",
    "# val_length = 13451\n",
    "# test_length = 22633\n",
    "\n",
    "n_features = 50\n",
    "random_seed = 42\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def create_model(n_units1=64, n_units2=32):\n",
    "    model = Sequential()\n",
    "    inputs = Input(shape=(seq_len, n_features), dtype=float)\n",
    "    mask = Masking()(inputs)\n",
    "    lstm_1 = LSTM(n_units1, return_sequences=True)(mask)\n",
    "    lstm_2 = LSTM(n_units2, return_sequences=True)(lstm_1)\n",
    "    lstm_2 = Dropout(0.3)(lstm_2)\n",
    "    modes = lstm_2\n",
    "    output = [TimeDistributed(Dense(1), name=name)(modes) for i, name in enumerate(headers)]\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    rmsprop = RMSprop(lr=0.0001)\n",
    "    model.compile(optimizer=rmsprop, loss=ccc_loss, metrics=[ccc])\n",
    "    return model\n",
    "#\n",
    "# def create_arousal(n_units1=64, n_units2=32, dropout, bidirection=False):\n",
    "#     a_input = Input(shape=(time_step, n_features), dtype=float, name='arousal_model_input')\n",
    "#     mask = Masking()(a_input)\n",
    "#     if bidirection:\n",
    "#         a_lstm1 = Bidirectional(LSTM(n_units1, return_sequences=True))(mask)\n",
    "#     else:\n",
    "#         a_lstm1 = LSTM(n_units1, return_sequences=True)(mask)\n",
    "#     a_lstm1 = Dropout(dropout)(a_lstm1)\n",
    "#     if bidirection:\n",
    "#         a_lstm2 = Bidirectional(LSTM(n_units2, return_sequences=False)(a_lstm1))\n",
    "#     else:\n",
    "#         a_lstm2 = LSTM(n_units2, return_sequences=False)(a_lstm1)\n",
    "#     a_lstm2 = Dropout(dropout)(a_lstm2)\n",
    "#     a_dense = Dense(\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f00a9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 388, 50)]    0           []                               \n",
      "                                                                                                  \n",
      " masking_4 (Masking)            (None, 388, 50)      0           ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_8 (LSTM)                  (None, 388, 256)     314368      ['masking_4[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_9 (LSTM)                  (None, 388, 256)     525312      ['lstm_8[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 388, 256)     0           ['lstm_9[0][0]']                 \n",
      "                                                                                                  \n",
      " Arousal (TimeDistributed)      (None, 388, 1)       257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Valence (TimeDistributed)      (None, 388, 1)       257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Dominance (TimeDistributed)    (None, 388, 1)       257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 840,451\n",
      "Trainable params: 840,451\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model(n_units1=256, n_units2=256)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c3d4cca-9de2-4ec0-a419-93fd35727da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mtl(y):\n",
    "    y_dta = []\n",
    "    index = [0,1,2]\n",
    "    for i in index:\n",
    "        dim = np.empty((len(y), seq_len, 1))\n",
    "        dim[:,:,0] = y[:,:,i]\n",
    "        y_dta.append(dim)\n",
    "    return y_dta\n",
    "\n",
    "y_train_dta = transform_mtl(y_train_scaled)\n",
    "y_val_dta = transform_mtl(y_val_scaled)\n",
    "y_test_dta = transform_mtl(y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4fb1de9f-1166-44b2-b7da-f94878d5de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "84/84 [==============================] - 11s 61ms/step - loss: 1.6365 - Arousal_loss: 0.4358 - Valence_loss: 0.8363 - Dominance_loss: 0.3645 - Arousal_ccc: 0.5642 - Valence_ccc: 0.1637 - Dominance_ccc: 0.6355 - val_loss: 1.3843 - val_Arousal_loss: 0.2637 - val_Valence_loss: 0.9324 - val_Dominance_loss: 0.1882 - val_Arousal_ccc: 0.7363 - val_Valence_ccc: 0.0676 - val_Dominance_ccc: 0.8118\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 1.2399 - Arousal_loss: 0.2691 - Valence_loss: 0.7665 - Dominance_loss: 0.2043 - Arousal_ccc: 0.7309 - Valence_ccc: 0.2335 - Dominance_ccc: 0.7957 - val_loss: 1.2801 - val_Arousal_loss: 0.2104 - val_Valence_loss: 0.9306 - val_Dominance_loss: 0.1392 - val_Arousal_ccc: 0.7896 - val_Valence_ccc: 0.0694 - val_Dominance_ccc: 0.8608\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 1.1086 - Arousal_loss: 0.2197 - Valence_loss: 0.7327 - Dominance_loss: 0.1562 - Arousal_ccc: 0.7803 - Valence_ccc: 0.2673 - Dominance_ccc: 0.8438 - val_loss: 1.3343 - val_Arousal_loss: 0.1805 - val_Valence_loss: 0.9667 - val_Dominance_loss: 0.1870 - val_Arousal_ccc: 0.8195 - val_Valence_ccc: 0.0333 - val_Dominance_ccc: 0.8130\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 1.0375 - Arousal_loss: 0.1974 - Valence_loss: 0.7040 - Dominance_loss: 0.1361 - Arousal_ccc: 0.8026 - Valence_ccc: 0.2960 - Dominance_ccc: 0.8639 - val_loss: 1.2819 - val_Arousal_loss: 0.1898 - val_Valence_loss: 0.9700 - val_Dominance_loss: 0.1222 - val_Arousal_ccc: 0.8102 - val_Valence_ccc: 0.0300 - val_Dominance_ccc: 0.8778\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.9829 - Arousal_loss: 0.1854 - Valence_loss: 0.6731 - Dominance_loss: 0.1244 - Arousal_ccc: 0.8146 - Valence_ccc: 0.3269 - Dominance_ccc: 0.8756 - val_loss: 1.2490 - val_Arousal_loss: 0.1672 - val_Valence_loss: 0.9627 - val_Dominance_loss: 0.1191 - val_Arousal_ccc: 0.8328 - val_Valence_ccc: 0.0373 - val_Dominance_ccc: 0.8809\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.9359 - Arousal_loss: 0.1786 - Valence_loss: 0.6374 - Dominance_loss: 0.1199 - Arousal_ccc: 0.8214 - Valence_ccc: 0.3626 - Dominance_ccc: 0.8801 - val_loss: 1.2289 - val_Arousal_loss: 0.1663 - val_Valence_loss: 0.9473 - val_Dominance_loss: 0.1153 - val_Arousal_ccc: 0.8337 - val_Valence_ccc: 0.0527 - val_Dominance_ccc: 0.8847\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.9015 - Arousal_loss: 0.1748 - Valence_loss: 0.6115 - Dominance_loss: 0.1152 - Arousal_ccc: 0.8252 - Valence_ccc: 0.3885 - Dominance_ccc: 0.8848 - val_loss: 1.2301 - val_Arousal_loss: 0.1642 - val_Valence_loss: 0.9491 - val_Dominance_loss: 0.1168 - val_Arousal_ccc: 0.8358 - val_Valence_ccc: 0.0509 - val_Dominance_ccc: 0.8832\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.8774 - Arousal_loss: 0.1727 - Valence_loss: 0.5917 - Dominance_loss: 0.1130 - Arousal_ccc: 0.8273 - Valence_ccc: 0.4083 - Dominance_ccc: 0.8870 - val_loss: 1.2192 - val_Arousal_loss: 0.1625 - val_Valence_loss: 0.9427 - val_Dominance_loss: 0.1140 - val_Arousal_ccc: 0.8375 - val_Valence_ccc: 0.0573 - val_Dominance_ccc: 0.8860\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.8567 - Arousal_loss: 0.1703 - Valence_loss: 0.5758 - Dominance_loss: 0.1106 - Arousal_ccc: 0.8297 - Valence_ccc: 0.4242 - Dominance_ccc: 0.8894 - val_loss: 1.2345 - val_Arousal_loss: 0.1657 - val_Valence_loss: 0.9488 - val_Dominance_loss: 0.1201 - val_Arousal_ccc: 0.8343 - val_Valence_ccc: 0.0512 - val_Dominance_ccc: 0.8799\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.8491 - Arousal_loss: 0.1677 - Valence_loss: 0.5729 - Dominance_loss: 0.1085 - Arousal_ccc: 0.8323 - Valence_ccc: 0.4271 - Dominance_ccc: 0.8915 - val_loss: 1.2238 - val_Arousal_loss: 0.1621 - val_Valence_loss: 0.9452 - val_Dominance_loss: 0.1165 - val_Arousal_ccc: 0.8379 - val_Valence_ccc: 0.0548 - val_Dominance_ccc: 0.8835\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.8374 - Arousal_loss: 0.1677 - Valence_loss: 0.5621 - Dominance_loss: 0.1075 - Arousal_ccc: 0.8323 - Valence_ccc: 0.4379 - Dominance_ccc: 0.8925 - val_loss: 1.2327 - val_Arousal_loss: 0.1632 - val_Valence_loss: 0.9524 - val_Dominance_loss: 0.1171 - val_Arousal_ccc: 0.8368 - val_Valence_ccc: 0.0476 - val_Dominance_ccc: 0.8829\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.8251 - Arousal_loss: 0.1652 - Valence_loss: 0.5530 - Dominance_loss: 0.1069 - Arousal_ccc: 0.8348 - Valence_ccc: 0.4470 - Dominance_ccc: 0.8931 - val_loss: 1.2189 - val_Arousal_loss: 0.1604 - val_Valence_loss: 0.9418 - val_Dominance_loss: 0.1166 - val_Arousal_ccc: 0.8396 - val_Valence_ccc: 0.0582 - val_Dominance_ccc: 0.8834\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.8123 - Arousal_loss: 0.1641 - Valence_loss: 0.5437 - Dominance_loss: 0.1045 - Arousal_ccc: 0.8359 - Valence_ccc: 0.4563 - Dominance_ccc: 0.8955 - val_loss: 1.2241 - val_Arousal_loss: 0.1706 - val_Valence_loss: 0.9342 - val_Dominance_loss: 0.1193 - val_Arousal_ccc: 0.8294 - val_Valence_ccc: 0.0658 - val_Dominance_ccc: 0.8807\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7982 - Arousal_loss: 0.1642 - Valence_loss: 0.5299 - Dominance_loss: 0.1041 - Arousal_ccc: 0.8358 - Valence_ccc: 0.4701 - Dominance_ccc: 0.8959 - val_loss: 1.2162 - val_Arousal_loss: 0.1594 - val_Valence_loss: 0.9404 - val_Dominance_loss: 0.1165 - val_Arousal_ccc: 0.8406 - val_Valence_ccc: 0.0596 - val_Dominance_ccc: 0.8835\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7870 - Arousal_loss: 0.1623 - Valence_loss: 0.5213 - Dominance_loss: 0.1034 - Arousal_ccc: 0.8377 - Valence_ccc: 0.4787 - Dominance_ccc: 0.8966 - val_loss: 1.2070 - val_Arousal_loss: 0.1633 - val_Valence_loss: 0.9281 - val_Dominance_loss: 0.1156 - val_Arousal_ccc: 0.8367 - val_Valence_ccc: 0.0719 - val_Dominance_ccc: 0.8844\n",
      "Epoch 16/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7792 - Arousal_loss: 0.1621 - Valence_loss: 0.5146 - Dominance_loss: 0.1025 - Arousal_ccc: 0.8379 - Valence_ccc: 0.4854 - Dominance_ccc: 0.8975 - val_loss: 1.2154 - val_Arousal_loss: 0.1570 - val_Valence_loss: 0.9419 - val_Dominance_loss: 0.1165 - val_Arousal_ccc: 0.8430 - val_Valence_ccc: 0.0581 - val_Dominance_ccc: 0.8835\n",
      "Epoch 17/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7727 - Arousal_loss: 0.1613 - Valence_loss: 0.5090 - Dominance_loss: 0.1025 - Arousal_ccc: 0.8387 - Valence_ccc: 0.4910 - Dominance_ccc: 0.8975 - val_loss: 1.2000 - val_Arousal_loss: 0.1572 - val_Valence_loss: 0.9292 - val_Dominance_loss: 0.1136 - val_Arousal_ccc: 0.8428 - val_Valence_ccc: 0.0708 - val_Dominance_ccc: 0.8864\n",
      "Epoch 18/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7639 - Arousal_loss: 0.1596 - Valence_loss: 0.5027 - Dominance_loss: 0.1015 - Arousal_ccc: 0.8404 - Valence_ccc: 0.4973 - Dominance_ccc: 0.8985 - val_loss: 1.2089 - val_Arousal_loss: 0.1600 - val_Valence_loss: 0.9359 - val_Dominance_loss: 0.1130 - val_Arousal_ccc: 0.8400 - val_Valence_ccc: 0.0641 - val_Dominance_ccc: 0.8870\n",
      "Epoch 19/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7560 - Arousal_loss: 0.1590 - Valence_loss: 0.4963 - Dominance_loss: 0.1006 - Arousal_ccc: 0.8410 - Valence_ccc: 0.5037 - Dominance_ccc: 0.8994 - val_loss: 1.1990 - val_Arousal_loss: 0.1558 - val_Valence_loss: 0.9314 - val_Dominance_loss: 0.1119 - val_Arousal_ccc: 0.8442 - val_Valence_ccc: 0.0686 - val_Dominance_ccc: 0.8881\n",
      "Epoch 20/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7472 - Arousal_loss: 0.1586 - Valence_loss: 0.4887 - Dominance_loss: 0.0999 - Arousal_ccc: 0.8414 - Valence_ccc: 0.5113 - Dominance_ccc: 0.9001 - val_loss: 1.2056 - val_Arousal_loss: 0.1572 - val_Valence_loss: 0.9360 - val_Dominance_loss: 0.1124 - val_Arousal_ccc: 0.8428 - val_Valence_ccc: 0.0640 - val_Dominance_ccc: 0.8876\n",
      "Epoch 21/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7388 - Arousal_loss: 0.1579 - Valence_loss: 0.4807 - Dominance_loss: 0.1002 - Arousal_ccc: 0.8421 - Valence_ccc: 0.5193 - Dominance_ccc: 0.8998 - val_loss: 1.2173 - val_Arousal_loss: 0.1640 - val_Valence_loss: 0.9367 - val_Dominance_loss: 0.1166 - val_Arousal_ccc: 0.8360 - val_Valence_ccc: 0.0633 - val_Dominance_ccc: 0.8834\n",
      "Epoch 22/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7354 - Arousal_loss: 0.1569 - Valence_loss: 0.4789 - Dominance_loss: 0.0996 - Arousal_ccc: 0.8431 - Valence_ccc: 0.5211 - Dominance_ccc: 0.9004 - val_loss: 1.2043 - val_Arousal_loss: 0.1587 - val_Valence_loss: 0.9285 - val_Dominance_loss: 0.1171 - val_Arousal_ccc: 0.8413 - val_Valence_ccc: 0.0715 - val_Dominance_ccc: 0.8829\n",
      "Epoch 23/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7349 - Arousal_loss: 0.1573 - Valence_loss: 0.4789 - Dominance_loss: 0.0987 - Arousal_ccc: 0.8427 - Valence_ccc: 0.5211 - Dominance_ccc: 0.9013 - val_loss: 1.1933 - val_Arousal_loss: 0.1547 - val_Valence_loss: 0.9270 - val_Dominance_loss: 0.1116 - val_Arousal_ccc: 0.8453 - val_Valence_ccc: 0.0730 - val_Dominance_ccc: 0.8884\n",
      "Epoch 24/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7254 - Arousal_loss: 0.1564 - Valence_loss: 0.4706 - Dominance_loss: 0.0984 - Arousal_ccc: 0.8436 - Valence_ccc: 0.5294 - Dominance_ccc: 0.9016 - val_loss: 1.1894 - val_Arousal_loss: 0.1551 - val_Valence_loss: 0.9222 - val_Dominance_loss: 0.1121 - val_Arousal_ccc: 0.8449 - val_Valence_ccc: 0.0778 - val_Dominance_ccc: 0.8879\n",
      "Epoch 25/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7223 - Arousal_loss: 0.1557 - Valence_loss: 0.4686 - Dominance_loss: 0.0980 - Arousal_ccc: 0.8443 - Valence_ccc: 0.5314 - Dominance_ccc: 0.9020 - val_loss: 1.1972 - val_Arousal_loss: 0.1561 - val_Valence_loss: 0.9281 - val_Dominance_loss: 0.1130 - val_Arousal_ccc: 0.8439 - val_Valence_ccc: 0.0719 - val_Dominance_ccc: 0.8870\n",
      "Epoch 26/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7123 - Arousal_loss: 0.1552 - Valence_loss: 0.4599 - Dominance_loss: 0.0972 - Arousal_ccc: 0.8448 - Valence_ccc: 0.5401 - Dominance_ccc: 0.9028 - val_loss: 1.2057 - val_Arousal_loss: 0.1561 - val_Valence_loss: 0.9354 - val_Dominance_loss: 0.1141 - val_Arousal_ccc: 0.8439 - val_Valence_ccc: 0.0646 - val_Dominance_ccc: 0.8859\n",
      "Epoch 27/50\n",
      "84/84 [==============================] - 3s 39ms/step - loss: 0.7091 - Arousal_loss: 0.1553 - Valence_loss: 0.4568 - Dominance_loss: 0.0970 - Arousal_ccc: 0.8447 - Valence_ccc: 0.5432 - Dominance_ccc: 0.9030 - val_loss: 1.2007 - val_Arousal_loss: 0.1568 - val_Valence_loss: 0.9266 - val_Dominance_loss: 0.1173 - val_Arousal_ccc: 0.8432 - val_Valence_ccc: 0.0734 - val_Dominance_ccc: 0.8827\n",
      "Epoch 28/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.7051 - Arousal_loss: 0.1543 - Valence_loss: 0.4545 - Dominance_loss: 0.0963 - Arousal_ccc: 0.8457 - Valence_ccc: 0.5455 - Dominance_ccc: 0.9037 - val_loss: 1.1887 - val_Arousal_loss: 0.1524 - val_Valence_loss: 0.9254 - val_Dominance_loss: 0.1108 - val_Arousal_ccc: 0.8476 - val_Valence_ccc: 0.0746 - val_Dominance_ccc: 0.8892\n",
      "Epoch 29/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6990 - Arousal_loss: 0.1539 - Valence_loss: 0.4484 - Dominance_loss: 0.0967 - Arousal_ccc: 0.8461 - Valence_ccc: 0.5516 - Dominance_ccc: 0.9033 - val_loss: 1.1973 - val_Arousal_loss: 0.1528 - val_Valence_loss: 0.9292 - val_Dominance_loss: 0.1153 - val_Arousal_ccc: 0.8472 - val_Valence_ccc: 0.0708 - val_Dominance_ccc: 0.8847\n",
      "Epoch 30/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6940 - Arousal_loss: 0.1539 - Valence_loss: 0.4437 - Dominance_loss: 0.0965 - Arousal_ccc: 0.8461 - Valence_ccc: 0.5563 - Dominance_ccc: 0.9035 - val_loss: 1.2034 - val_Arousal_loss: 0.1524 - val_Valence_loss: 0.9390 - val_Dominance_loss: 0.1120 - val_Arousal_ccc: 0.8476 - val_Valence_ccc: 0.0610 - val_Dominance_ccc: 0.8880\n",
      "Epoch 31/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6923 - Arousal_loss: 0.1528 - Valence_loss: 0.4437 - Dominance_loss: 0.0959 - Arousal_ccc: 0.8472 - Valence_ccc: 0.5563 - Dominance_ccc: 0.9041 - val_loss: 1.1831 - val_Arousal_loss: 0.1525 - val_Valence_loss: 0.9169 - val_Dominance_loss: 0.1137 - val_Arousal_ccc: 0.8475 - val_Valence_ccc: 0.0831 - val_Dominance_ccc: 0.8863\n",
      "Epoch 32/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6923 - Arousal_loss: 0.1528 - Valence_loss: 0.4441 - Dominance_loss: 0.0953 - Arousal_ccc: 0.8472 - Valence_ccc: 0.5559 - Dominance_ccc: 0.9047 - val_loss: 1.2057 - val_Arousal_loss: 0.1561 - val_Valence_loss: 0.9342 - val_Dominance_loss: 0.1155 - val_Arousal_ccc: 0.8439 - val_Valence_ccc: 0.0658 - val_Dominance_ccc: 0.8845\n",
      "Epoch 33/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6789 - Arousal_loss: 0.1524 - Valence_loss: 0.4317 - Dominance_loss: 0.0948 - Arousal_ccc: 0.8476 - Valence_ccc: 0.5683 - Dominance_ccc: 0.9052 - val_loss: 1.1917 - val_Arousal_loss: 0.1537 - val_Valence_loss: 0.9249 - val_Dominance_loss: 0.1131 - val_Arousal_ccc: 0.8463 - val_Valence_ccc: 0.0751 - val_Dominance_ccc: 0.8869\n",
      "Epoch 34/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6800 - Arousal_loss: 0.1520 - Valence_loss: 0.4334 - Dominance_loss: 0.0946 - Arousal_ccc: 0.8480 - Valence_ccc: 0.5666 - Dominance_ccc: 0.9054 - val_loss: 1.2006 - val_Arousal_loss: 0.1526 - val_Valence_loss: 0.9352 - val_Dominance_loss: 0.1127 - val_Arousal_ccc: 0.8474 - val_Valence_ccc: 0.0648 - val_Dominance_ccc: 0.8873\n",
      "Epoch 35/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6800 - Arousal_loss: 0.1510 - Valence_loss: 0.4346 - Dominance_loss: 0.0944 - Arousal_ccc: 0.8490 - Valence_ccc: 0.5654 - Dominance_ccc: 0.9056 - val_loss: 1.1864 - val_Arousal_loss: 0.1510 - val_Valence_loss: 0.9237 - val_Dominance_loss: 0.1116 - val_Arousal_ccc: 0.8490 - val_Valence_ccc: 0.0763 - val_Dominance_ccc: 0.8884\n",
      "Epoch 36/50\n",
      "84/84 [==============================] - 3s 38ms/step - loss: 0.6741 - Arousal_loss: 0.1508 - Valence_loss: 0.4294 - Dominance_loss: 0.0939 - Arousal_ccc: 0.8492 - Valence_ccc: 0.5706 - Dominance_ccc: 0.9061 - val_loss: 1.1969 - val_Arousal_loss: 0.1543 - val_Valence_loss: 0.9227 - val_Dominance_loss: 0.1198 - val_Arousal_ccc: 0.8457 - val_Valence_ccc: 0.0773 - val_Dominance_ccc: 0.8802\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "history = model.fit(x_train_scaled, y_train_dta, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,                     \n",
    "                    validation_data=(x_val_scaled, y_val_dta), \n",
    "                    callbacks=[callback])\n",
    "# plot_learningCurve(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bd54dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features shape (3410, 388, 50)\n",
      "Combined labels shape and target num (3410, 388, 1) and 3\n"
     ]
    }
   ],
   "source": [
    "x_combined = np.concatenate([x_train_scaled, x_val_scaled])\n",
    "y_combined = []\n",
    "for i in range(3):\n",
    "    temp = np.concatenate([y_train_dta[i], y_val_dta[i]])\n",
    "    y_combined.append(temp)\n",
    "print(\"Combined features shape %s\" % (str(x_combined.shape)))\n",
    "print(\"Combined labels shape and target num %s and %s\" % (str(y_combined[0].shape), str(len(y_combined))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04a893-8ffe-464c-b0cf-782dd14c9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_units1=256, n_units2=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084594e-9150-4f33-82f7-aaf958128497",
   "metadata": {},
   "source": [
    "### Linguistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59a3d2b2-55f2-47d6-ab60-d03987106dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /users/k21134342/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f059d169-0731-412e-847e-85cbd031f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    punctuation = [i for i in ',./\\\\;:\\'@#~[{]}=+-_)(*&^%$£\"!`)]']\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    \n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = \"\".join([\" \" if t in punctuation else t for t in text]).lower()\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5f9e2a29-c2c0-4d0e-8e7b-92b58ff7d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_path = './transcripts/'\n",
    "sections = ['train/', 'validation/', 'test/']\n",
    "\n",
    "for i, section in enumerate(sections):\n",
    "    files = fnmatch.filter(os.listdir(transcripts_path+section), '*.txt')\n",
    "    files.sort()\n",
    "    text = []\n",
    "    filename = []\n",
    "    for file in files:\n",
    "        inst = file.split('.')[0]\n",
    "        if 'MSP-PODCAST_0153' in inst or 'MSP-PODCAST_1188_0020' in inst:\n",
    "            continue\n",
    "        filename.append(inst)\n",
    "        with open(transcripts_path+section+file) as f:\n",
    "            lines = f.readlines()\n",
    "        if len(lines) == 0:\n",
    "            text.append('')\n",
    "        else:\n",
    "            text.append(lines[0])\n",
    "    if i == 0:\n",
    "        df_train = pd.DataFrame({'Filename': filename, 'txt':text})\n",
    "        \n",
    "    elif i == 1:\n",
    "        df_val = pd.DataFrame({'Filename': filename, 'txt':text})\n",
    "        \n",
    "    else:\n",
    "        df_test = pd.DataFrame({'Filename': filename, 'txt':text})\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ca1710dc-1add-45ac-a991-f524417e1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned train text: 2702\n",
      "Cleaned validation text: 749\n",
      "Cleaned test text: 1503\n",
      "Cleaned all text: 4954\n"
     ]
    }
   ],
   "source": [
    "train_cleaned = df_train['txt'].apply(clean_text)\n",
    "val_cleaned = df_val['txt'].apply(clean_text)\n",
    "test_cleaned = df_test['txt'].apply(clean_text)\n",
    "combined_cleaned = pd.concat([train_cleaned, val_cleaned, test_cleaned], ignore_index=True)\n",
    "\n",
    "print(\"Cleaned train text: %d\" % (len(train_cleaned)))\n",
    "print(\"Cleaned validation text: %d\" % (len(val_cleaned)))\n",
    "print(\"Cleaned test text: %d\" % (len(test_cleaned)))\n",
    "print(\"Cleaned all text: %d\" % (len(combined_cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4c560c99-3510-47e6-a374-52fed890fafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8199"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "for i in range(len(combined_cleaned)):\n",
    "    text = combined_cleaned[i]\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        vocab.add(token)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "855e9673-e6a5-4c23-9db8-07053f2208c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 395\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "sentence_len = [len(sent.split()) for sent in combined_cleaned.tolist()]\n",
    "sent_len = max(sentence_len)\n",
    "print('Max sentence length: %d' % (sent_len))\n",
    "text_vec = TextVectorization(max_tokens=vocab_size, \n",
    "                             pad_to_max_tokens=True, \n",
    "                             output_sequence_length=sent_len,\n",
    "                             output_mode='int')\n",
    "text_vec.adapt(train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f5fd0ee5-538d-43be-a517-69f6c3fe70c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "# loading pre-trained weights and build embedding layer\n",
    "embeddings_index = dict()\n",
    "embedding_dim = 300\n",
    "f = open('embeddings/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ef9067a7-5966-4141-a6db-c6ef29f7a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size+ 1, embedding_dim))\n",
    "for i, word in enumerate(text_vec.get_vocabulary()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f0f6d225-03e0-46e1-9ba4-6c13e671ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size + 1, \n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=seq_len,\n",
    "                            trainable=False,\n",
    "                            name='gloVe embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d179656-95c8-4667-950a-1b210af8f540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21aa187-3d33-4961-8a7a-122147582cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
