{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ae74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfaa248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = 25253\n",
    "# validation_len = 9471\n",
    "# test_len = 13794\n",
    "train_length = 2684\n",
    "val_length = 726\n",
    "test_length = 1498\n",
    "\n",
    "def load_features(filename,\n",
    "                  skip_header=True,\n",
    "                  skip_instname=True,\n",
    "                  delim=' ',\n",
    "                  num_lines=0):\n",
    "    if num_lines == 0:\n",
    "        num_lines = get_num_lines(filename, skip_header)\n",
    "\n",
    "    data = np.empty(\n",
    "        (num_lines, 50), float)\n",
    "\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        if skip_header:\n",
    "            next(csv_file)\n",
    "        c = 0\n",
    "        for line in tqdm(csv_file):\n",
    "            offset = 0\n",
    "            if skip_instname:\n",
    "                offset = line.find(delim) + 1\n",
    "            data[c, :] = np.fromstring(line[offset:], dtype=float, sep=delim)\n",
    "            c += 1\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_batch_features(filename, start_index=0, amount=0):\n",
    "    delim = ' '\n",
    "    \n",
    "    data = np.empty((amount, 25), float)\n",
    "    \n",
    "    with open(filename, 'r') as csv_file:\n",
    "        for i, line in tqdm(enumerate(csv_file)):\n",
    "            if i < start_index:\n",
    "                continue\n",
    "            if i >= start_index + amount:\n",
    "                break\n",
    "            index = i - start_index\n",
    "            data[index, :] = np.fromstring(line, dtype=float, sep=delim)\n",
    "    return data\n",
    "    \n",
    "def load_batch_labels(filename, start_index=1, amount=0):\n",
    "    labels = np.empty((amount, 3), float)\n",
    "    delim = ','\n",
    "    \n",
    "    with open(filename, 'r') as csv_file:\n",
    "        for i, line in tqdm(enumerate(csv_file)):\n",
    "            if i < start_index:\n",
    "                continue\n",
    "            if i >= start_index + amount:\n",
    "                break\n",
    "            cols = np.fromstring(line, dtype=float, sep=delim)\n",
    "            index = i - start_index\n",
    "            labels[index, :] = cols[1:]\n",
    "    return labels\n",
    "    \n",
    "def get_num_lines(filename, skip_header):\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        if skip_header:\n",
    "            next(csv_file)\n",
    "        c = 0\n",
    "        for line in csv_file:\n",
    "            c += 1\n",
    "    return c\n",
    "\n",
    "def load_labels(filename,\n",
    "                  skip_header=True,\n",
    "                  skip_instname=True,\n",
    "                  delim=' ',\n",
    "                  num_lines=0):\n",
    "    if num_lines == 0:\n",
    "        num_lines = get_num_lines(filename, skip_header)\n",
    "\n",
    "    data = np.empty(\n",
    "        (num_lines, 3), float)\n",
    "\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        if skip_header:\n",
    "            next(csv_file)\n",
    "        c = 0\n",
    "        for line in tqdm(csv_file):\n",
    "            offset = 0\n",
    "            if skip_instname:\n",
    "                offset = line.find(delim) + 1\n",
    "            data[c, :] = np.fromstring(line[offset:], dtype=float, sep=delim)\n",
    "            c += 1\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def get_scaler(x):\n",
    "    x_scaler = StandardScaler()\n",
    "    x_scaler.fit(x)\n",
    "  \n",
    "    return x_scaler\n",
    "\n",
    "def scale_data(scaler, data):\n",
    "    if data.ndim > 2:\n",
    "        data = data.reshape(-1, data.shape[2])\n",
    "    scaled = scaler.transform(data)\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "def ccc(gold, pred):\n",
    "    gold       = K.squeeze(gold, axis=-1)\n",
    "    pred       = K.squeeze(pred, axis=-1)\n",
    "    gold_mean  = K.mean(gold, axis=-1, keepdims=True)\n",
    "    pred_mean  = K.mean(pred, axis=-1, keepdims=True)\n",
    "    covariance = (gold-gold_mean)*(pred-pred_mean)\n",
    "    gold_var   = K.mean(K.square(gold-gold_mean), axis=-1,  keepdims=True)\n",
    "    pred_var   = K.mean(K.square(pred-pred_mean), axis=-1, keepdims=True)\n",
    "    ccc        = K.constant(2.) * covariance / (gold_var + pred_var + K.square(gold_mean - pred_mean) + K.epsilon())\n",
    "    return ccc\n",
    "\n",
    "def ccc_loss(gold, pred):\n",
    "    ccc_loss = K.constant(1.) - ccc(gold, pred)\n",
    "    return ccc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d39c15",
   "metadata": {},
   "source": [
    "### Batch loading to train LSTM-RNN\n",
    "\n",
    "- First, load all data to get scalers that covers for each partition data\n",
    "- Batching the data to train\n",
    "- Batching the data to validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f69b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1041392it [00:09, 107926.84it/s]\n",
      "1041392it [00:02, 384258.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished, Scaling...\n",
      "x_train shape: (2684, 388, 50)\n",
      "y_train shape: (2684, 388, 3)\n",
      "End of loading and preprocessing training samples\n",
      "Loading validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "281688it [00:02, 108125.03it/s]\n",
      "281688it [00:00, 339885.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished, Scaling...\n",
      "x_validation shape: (726, 388, 50)\n",
      "y_validation shape: (726, 388, 3)\n",
      "End of loading and preprocessing validation samples\n",
      "Loading testing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "581224it [00:05, 110949.91it/s]\n",
      "581224it [00:01, 374315.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finished, Scaling...\n",
      "x_test shape: (1498, 388, 50)\n",
      "y_test shape: (1498, 388, 3)\n",
      "End of loading and preprocessing test samples\n"
     ]
    }
   ],
   "source": [
    "data_path = './Functional_features/'\n",
    "\n",
    "seq_len = 388\n",
    "n_features = 25\n",
    "\n",
    "\n",
    "# load all data to get a scaler that covers all data\n",
    "print(\"Loading training samples...\")\n",
    "x_train = load_features(data_path+'train.txt', skip_header=False, skip_instname=False)\n",
    "train_labels = load_labels(data_path+'train_labels.txt', skip_header=False, skip_instname=False)\n",
    "y_train = train_labels.reshape((train_length, seq_len, 3))\n",
    "\n",
    "print(\"Loading finished, Scaling...\")\n",
    "x_scaler = get_scaler(x_train)\n",
    "\n",
    "# Scaling acoustic features\n",
    "x_train_scaled = scale_data(x_scaler, x_train)\n",
    "# Scaling labels from [-100,100] to [-1, 1]\n",
    "f = lambda x: x * 0.01\n",
    "y_train_scaled = f(y_train)\n",
    "x_train_scaled = x_train_scaled.reshape((train_length, seq_len, n_features * 2))\n",
    "print('x_train shape:', x_train_scaled.shape)\n",
    "print('y_train shape:', y_train_scaled.shape)\n",
    "print(\"End of loading and preprocessing training samples\")\n",
    "\n",
    "print(\"Loading validation samples...\")\n",
    "x_validation = load_features(data_path+'validation.txt', skip_header = False, skip_instname=False)\n",
    "val_labels = load_labels(data_path+'validation_labels.txt', skip_header=False, skip_instname=False)\n",
    "y_validation = val_labels.reshape((val_length, seq_len, 3))\n",
    "print(\"Loading finished, Scaling...\")\n",
    "x_val_scaled = scale_data(x_scaler, x_validation)\n",
    "y_val_scaled = f(y_validation)\n",
    "x_val_scaled = x_val_scaled.reshape((val_length, seq_len, n_features * 2))\n",
    "\n",
    "print('x_validation shape:', x_val_scaled.shape)\n",
    "print('y_validation shape:', y_val_scaled.shape)\n",
    "print(\"End of loading and preprocessing validation samples\")\n",
    "\n",
    "print(\"Loading testing samples...\")\n",
    "x_test = load_features(data_path+'test.txt', skip_header = False, skip_instname=False)\n",
    "test_labels = load_labels(data_path+'test_labels.txt', skip_header=False, skip_instname=False)\n",
    "y_test = test_labels.reshape((test_length, seq_len, 3))\n",
    "print(\"Loading finished, Scaling...\")\n",
    "x_test_scaled = scale_data(x_scaler, x_test)\n",
    "y_test_scaled = f(y_test)\n",
    "x_test_scaled = x_test_scaled.reshape((test_length, seq_len, n_features * 2))\n",
    "\n",
    "print('x_test shape:', x_test_scaled.shape)\n",
    "print('y_test shape:', y_test_scaled.shape)\n",
    "print(\"End of loading and preprocessing test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a161c73-9701-4ff4-ad61-cc24fc47aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = y_train.columns\n",
    "# print(headers)\n",
    "headers = ['Arousal', 'Valence', 'Dominance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbeed576-7055-44b8-b324-d2c887cd50f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2684"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13dc203",
   "metadata": {},
   "source": [
    "### Building RNN-LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3bb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model, save_model, load_model, Sequential\n",
    "from keras.layers import Input, Dense, Masking, LSTM, Dropout, TimeDistributed, Bidirectional, Flatten, Embedding, Conv1D, BatchNormalization, MaxPool1D\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.utils import set_random_seed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1e34a-531f-484a-a58e-8fe1b733ba38",
   "metadata": {},
   "source": [
    "### Multi-task learning\n",
    "\n",
    "three models are integrated and each model covers Arousal, Valence, Dominance respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40323eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_length = 25253\n",
    "# val_length = 9471\n",
    "# test_length = 13794\n",
    "# train_length = 41815\n",
    "# val_length = 13451\n",
    "# test_length = 22633\n",
    "\n",
    "n_features = 50\n",
    "random_seed = 42\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def create_model(n_units1=64, n_units2=32):\n",
    "    model = Sequential()\n",
    "    inputs = Input(shape=(seq_len, n_features), dtype=float)\n",
    "    mask = Masking()(inputs)\n",
    "    lstm_1 = LSTM(n_units1, return_sequences=True)(mask)\n",
    "    lstm_2 = LSTM(n_units2, return_sequences=True)(lstm_1)\n",
    "    lstm_2 = Dropout(0.3)(lstm_2)\n",
    "    modes = lstm_2\n",
    "    output = [TimeDistributed(Dense(1), name=name)(modes) for i, name in enumerate(headers)]\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    rmsprop = RMSprop(lr=0.0001)\n",
    "    model.compile(optimizer=rmsprop, loss=ccc_loss, metrics=[ccc])\n",
    "    return model\n",
    "#\n",
    "# def create_arousal(n_units1=64, n_units2=32, dropout, bidirection=False):\n",
    "#     a_input = Input(shape=(time_step, n_features), dtype=float, name='arousal_model_input')\n",
    "#     mask = Masking()(a_input)\n",
    "#     if bidirection:\n",
    "#         a_lstm1 = Bidirectional(LSTM(n_units1, return_sequences=True))(mask)\n",
    "#     else:\n",
    "#         a_lstm1 = LSTM(n_units1, return_sequences=True)(mask)\n",
    "#     a_lstm1 = Dropout(dropout)(a_lstm1)\n",
    "#     if bidirection:\n",
    "#         a_lstm2 = Bidirectional(LSTM(n_units2, return_sequences=False)(a_lstm1))\n",
    "#     else:\n",
    "#         a_lstm2 = LSTM(n_units2, return_sequences=False)(a_lstm1)\n",
    "#     a_lstm2 = Dropout(dropout)(a_lstm2)\n",
    "#     a_dense = Dense(\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00a9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 22:40:28.647772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:28.710258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:28.710522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:28.711331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-20 22:40:28.712335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:28.712708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:28.713021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:29.447987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:29.448233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:29.448444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-20 22:40:29.448626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10436 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:10:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 388, 50)]    0           []                               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, 388, 50)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 388, 256)     314368      ['masking[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 388, 256)     525312      ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 388, 256)     0           ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " Arousal (TimeDistributed)      (None, 388, 1)       257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Valence (TimeDistributed)      (None, 388, 1)       257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Dominance (TimeDistributed)    (None, 388, 1)       257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 840,451\n",
      "Trainable params: 840,451\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changhyun/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = create_model(n_units1=256, n_units2=256)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3d4cca-9de2-4ec0-a419-93fd35727da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_mtl(y):\n",
    "    y_dta = []\n",
    "    index = [0,1,2]\n",
    "    for i in index:\n",
    "        dim = np.empty((len(y), seq_len, 1))\n",
    "        dim[:,:,0] = y[:,:,i]\n",
    "        y_dta.append(dim)\n",
    "    return y_dta\n",
    "\n",
    "y_train_dta = transform_mtl(y_train_scaled)\n",
    "y_val_dta = transform_mtl(y_val_scaled)\n",
    "y_test_dta = transform_mtl(y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb1de9f-1166-44b2-b7da-f94878d5de27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-20 22:41:27.086131: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_LEGACY_VARIANT\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_24'\n",
      "2022-07-20 22:41:27.935758: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 18s 100ms/step - loss: 1.6532 - Arousal_loss: 0.4276 - Valence_loss: 0.8261 - Dominance_loss: 0.3995 - Arousal_ccc: 0.5724 - Valence_ccc: 0.1739 - Dominance_ccc: 0.6005 - val_loss: 1.5319 - val_Arousal_loss: 0.2836 - val_Valence_loss: 0.9342 - val_Dominance_loss: 0.3142 - val_Arousal_ccc: 0.7164 - val_Valence_ccc: 0.0658 - val_Dominance_ccc: 0.6858\n",
      "Epoch 2/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 1.2411 - Arousal_loss: 0.2657 - Valence_loss: 0.7641 - Dominance_loss: 0.2114 - Arousal_ccc: 0.7343 - Valence_ccc: 0.2359 - Dominance_ccc: 0.7886 - val_loss: 1.3046 - val_Arousal_loss: 0.1963 - val_Valence_loss: 0.9307 - val_Dominance_loss: 0.1775 - val_Arousal_ccc: 0.8037 - val_Valence_ccc: 0.0693 - val_Dominance_ccc: 0.8225\n",
      "Epoch 3/50\n",
      "84/84 [==============================] - 5s 61ms/step - loss: 1.1186 - Arousal_loss: 0.2202 - Valence_loss: 0.7375 - Dominance_loss: 0.1608 - Arousal_ccc: 0.7798 - Valence_ccc: 0.2625 - Dominance_ccc: 0.8392 - val_loss: 1.2466 - val_Arousal_loss: 0.1791 - val_Valence_loss: 0.9251 - val_Dominance_loss: 0.1424 - val_Arousal_ccc: 0.8209 - val_Valence_ccc: 0.0749 - val_Dominance_ccc: 0.8576\n",
      "Epoch 4/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 1.0513 - Arousal_loss: 0.1957 - Valence_loss: 0.7202 - Dominance_loss: 0.1354 - Arousal_ccc: 0.8043 - Valence_ccc: 0.2798 - Dominance_ccc: 0.8646 - val_loss: 1.2798 - val_Arousal_loss: 0.1769 - val_Valence_loss: 0.9309 - val_Dominance_loss: 0.1720 - val_Arousal_ccc: 0.8231 - val_Valence_ccc: 0.0691 - val_Dominance_ccc: 0.8280\n",
      "Epoch 5/50\n",
      "84/84 [==============================] - 5s 61ms/step - loss: 1.0057 - Arousal_loss: 0.1856 - Valence_loss: 0.6964 - Dominance_loss: 0.1237 - Arousal_ccc: 0.8144 - Valence_ccc: 0.3036 - Dominance_ccc: 0.8763 - val_loss: 1.2404 - val_Arousal_loss: 0.1650 - val_Valence_loss: 0.9589 - val_Dominance_loss: 0.1166 - val_Arousal_ccc: 0.8350 - val_Valence_ccc: 0.0411 - val_Dominance_ccc: 0.8834\n",
      "Epoch 6/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.9311 - Arousal_loss: 0.1779 - Valence_loss: 0.6362 - Dominance_loss: 0.1171 - Arousal_ccc: 0.8221 - Valence_ccc: 0.3638 - Dominance_ccc: 0.8829 - val_loss: 1.2264 - val_Arousal_loss: 0.1602 - val_Valence_loss: 0.9525 - val_Dominance_loss: 0.1138 - val_Arousal_ccc: 0.8398 - val_Valence_ccc: 0.0475 - val_Dominance_ccc: 0.8862\n",
      "Epoch 7/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.8963 - Arousal_loss: 0.1740 - Valence_loss: 0.6090 - Dominance_loss: 0.1133 - Arousal_ccc: 0.8260 - Valence_ccc: 0.3910 - Dominance_ccc: 0.8867 - val_loss: 1.2103 - val_Arousal_loss: 0.1607 - val_Valence_loss: 0.9334 - val_Dominance_loss: 0.1161 - val_Arousal_ccc: 0.8393 - val_Valence_ccc: 0.0666 - val_Dominance_ccc: 0.8839\n",
      "Epoch 8/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.8788 - Arousal_loss: 0.1720 - Valence_loss: 0.5957 - Dominance_loss: 0.1112 - Arousal_ccc: 0.8280 - Valence_ccc: 0.4043 - Dominance_ccc: 0.8888 - val_loss: 1.2341 - val_Arousal_loss: 0.1611 - val_Valence_loss: 0.9471 - val_Dominance_loss: 0.1259 - val_Arousal_ccc: 0.8389 - val_Valence_ccc: 0.0529 - val_Dominance_ccc: 0.8741\n",
      "Epoch 9/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.8601 - Arousal_loss: 0.1705 - Valence_loss: 0.5810 - Dominance_loss: 0.1086 - Arousal_ccc: 0.8295 - Valence_ccc: 0.4190 - Dominance_ccc: 0.8914 - val_loss: 1.2304 - val_Arousal_loss: 0.1645 - val_Valence_loss: 0.9484 - val_Dominance_loss: 0.1175 - val_Arousal_ccc: 0.8355 - val_Valence_ccc: 0.0516 - val_Dominance_ccc: 0.8825\n",
      "Epoch 10/50\n",
      "84/84 [==============================] - 5s 61ms/step - loss: 0.8399 - Arousal_loss: 0.1680 - Valence_loss: 0.5647 - Dominance_loss: 0.1072 - Arousal_ccc: 0.8320 - Valence_ccc: 0.4353 - Dominance_ccc: 0.8928 - val_loss: 1.2063 - val_Arousal_loss: 0.1579 - val_Valence_loss: 0.9362 - val_Dominance_loss: 0.1122 - val_Arousal_ccc: 0.8421 - val_Valence_ccc: 0.0638 - val_Dominance_ccc: 0.8878\n",
      "Epoch 11/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.8331 - Arousal_loss: 0.1660 - Valence_loss: 0.5612 - Dominance_loss: 0.1059 - Arousal_ccc: 0.8340 - Valence_ccc: 0.4388 - Dominance_ccc: 0.8941 - val_loss: 1.2331 - val_Arousal_loss: 0.1683 - val_Valence_loss: 0.9492 - val_Dominance_loss: 0.1156 - val_Arousal_ccc: 0.8317 - val_Valence_ccc: 0.0508 - val_Dominance_ccc: 0.8844\n",
      "Epoch 12/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.8226 - Arousal_loss: 0.1658 - Valence_loss: 0.5523 - Dominance_loss: 0.1044 - Arousal_ccc: 0.8342 - Valence_ccc: 0.4477 - Dominance_ccc: 0.8956 - val_loss: 1.2261 - val_Arousal_loss: 0.1630 - val_Valence_loss: 0.9482 - val_Dominance_loss: 0.1150 - val_Arousal_ccc: 0.8370 - val_Valence_ccc: 0.0518 - val_Dominance_ccc: 0.8850\n",
      "Epoch 13/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.8106 - Arousal_loss: 0.1638 - Valence_loss: 0.5423 - Dominance_loss: 0.1045 - Arousal_ccc: 0.8362 - Valence_ccc: 0.4577 - Dominance_ccc: 0.8955 - val_loss: 1.2212 - val_Arousal_loss: 0.1602 - val_Valence_loss: 0.9477 - val_Dominance_loss: 0.1132 - val_Arousal_ccc: 0.8398 - val_Valence_ccc: 0.0523 - val_Dominance_ccc: 0.8868\n",
      "Epoch 14/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.7976 - Arousal_loss: 0.1624 - Valence_loss: 0.5321 - Dominance_loss: 0.1031 - Arousal_ccc: 0.8376 - Valence_ccc: 0.4679 - Dominance_ccc: 0.8969 - val_loss: 1.2188 - val_Arousal_loss: 0.1596 - val_Valence_loss: 0.9424 - val_Dominance_loss: 0.1168 - val_Arousal_ccc: 0.8404 - val_Valence_ccc: 0.0576 - val_Dominance_ccc: 0.8832\n",
      "Epoch 15/50\n",
      "84/84 [==============================] - 5s 62ms/step - loss: 0.7865 - Arousal_loss: 0.1621 - Valence_loss: 0.5221 - Dominance_loss: 0.1023 - Arousal_ccc: 0.8379 - Valence_ccc: 0.4779 - Dominance_ccc: 0.8977 - val_loss: 1.2115 - val_Arousal_loss: 0.1616 - val_Valence_loss: 0.9298 - val_Dominance_loss: 0.1201 - val_Arousal_ccc: 0.8384 - val_Valence_ccc: 0.0702 - val_Dominance_ccc: 0.8799\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "history = model.fit(x_train_scaled, y_train_dta, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,                     \n",
    "                    validation_data=(x_val_scaled, y_val_dta), \n",
    "                    callbacks=[callback])\n",
    "# plot_learningCurve(history, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bd54dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features shape (3410, 388, 50)\n",
      "Combined labels shape and target num (3410, 388, 1) and 3\n"
     ]
    }
   ],
   "source": [
    "x_combined = np.concatenate([x_train_scaled, x_val_scaled])\n",
    "y_combined = []\n",
    "for i in range(3):\n",
    "    temp = np.concatenate([y_train_dta[i], y_val_dta[i]])\n",
    "    y_combined.append(temp)\n",
    "print(\"Combined features shape %s\" % (str(x_combined.shape)))\n",
    "print(\"Combined labels shape and target num %s and %s\" % (str(y_combined[0].shape), str(len(y_combined))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e04a893-8ffe-464c-b0cf-782dd14c9325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "107/107 [==============================] - 21s 96ms/step - loss: 1.6317 - Arousal_loss: 0.4123 - Valence_loss: 0.8425 - Dominance_loss: 0.3768 - Arousal_ccc: 0.5877 - Valence_ccc: 0.1575 - Dominance_ccc: 0.6232 - val_loss: 0.9502 - val_Arousal_loss: 0.1662 - val_Valence_loss: 0.6591 - val_Dominance_loss: 0.1249 - val_Arousal_ccc: 0.8338 - val_Valence_ccc: 0.3409 - val_Dominance_ccc: 0.8751\n",
      "Epoch 2/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 1.2177 - Arousal_loss: 0.2413 - Valence_loss: 0.7896 - Dominance_loss: 0.1868 - Arousal_ccc: 0.7587 - Valence_ccc: 0.2104 - Dominance_ccc: 0.8132 - val_loss: 0.9098 - val_Arousal_loss: 0.1415 - val_Valence_loss: 0.6765 - val_Dominance_loss: 0.0918 - val_Arousal_ccc: 0.8585 - val_Valence_ccc: 0.3235 - val_Dominance_ccc: 0.9082\n",
      "Epoch 3/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 1.1083 - Arousal_loss: 0.2003 - Valence_loss: 0.7659 - Dominance_loss: 0.1421 - Arousal_ccc: 0.7997 - Valence_ccc: 0.2341 - Dominance_ccc: 0.8579 - val_loss: 0.8573 - val_Arousal_loss: 0.1325 - val_Valence_loss: 0.6424 - val_Dominance_loss: 0.0824 - val_Arousal_ccc: 0.8675 - val_Valence_ccc: 0.3576 - val_Dominance_ccc: 0.9176\n",
      "Epoch 4/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 1.0549 - Arousal_loss: 0.1840 - Valence_loss: 0.7455 - Dominance_loss: 0.1254 - Arousal_ccc: 0.8160 - Valence_ccc: 0.2545 - Dominance_ccc: 0.8746 - val_loss: 0.9311 - val_Arousal_loss: 0.1268 - val_Valence_loss: 0.7199 - val_Dominance_loss: 0.0844 - val_Arousal_ccc: 0.8732 - val_Valence_ccc: 0.2801 - val_Dominance_ccc: 0.9156\n",
      "Epoch 5/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.9944 - Arousal_loss: 0.1750 - Valence_loss: 0.7014 - Dominance_loss: 0.1180 - Arousal_ccc: 0.8250 - Valence_ccc: 0.2986 - Dominance_ccc: 0.8820 - val_loss: 0.9160 - val_Arousal_loss: 0.1208 - val_Valence_loss: 0.7113 - val_Dominance_loss: 0.0839 - val_Arousal_ccc: 0.8792 - val_Valence_ccc: 0.2887 - val_Dominance_ccc: 0.9161\n",
      "Epoch 6/50\n",
      "107/107 [==============================] - 7s 65ms/step - loss: 0.9656 - Arousal_loss: 0.1717 - Valence_loss: 0.6799 - Dominance_loss: 0.1139 - Arousal_ccc: 0.8283 - Valence_ccc: 0.3201 - Dominance_ccc: 0.8861 - val_loss: 0.8860 - val_Arousal_loss: 0.1260 - val_Valence_loss: 0.6609 - val_Dominance_loss: 0.0991 - val_Arousal_ccc: 0.8740 - val_Valence_ccc: 0.3391 - val_Dominance_ccc: 0.9009\n",
      "Epoch 7/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.9486 - Arousal_loss: 0.1687 - Valence_loss: 0.6680 - Dominance_loss: 0.1119 - Arousal_ccc: 0.8313 - Valence_ccc: 0.3320 - Dominance_ccc: 0.8881 - val_loss: 0.8569 - val_Arousal_loss: 0.1206 - val_Valence_loss: 0.6531 - val_Dominance_loss: 0.0833 - val_Arousal_ccc: 0.8794 - val_Valence_ccc: 0.3469 - val_Dominance_ccc: 0.9167\n",
      "Epoch 8/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.9293 - Arousal_loss: 0.1660 - Valence_loss: 0.6534 - Dominance_loss: 0.1098 - Arousal_ccc: 0.8340 - Valence_ccc: 0.3466 - Dominance_ccc: 0.8902 - val_loss: 0.8577 - val_Arousal_loss: 0.1253 - val_Valence_loss: 0.6515 - val_Dominance_loss: 0.0809 - val_Arousal_ccc: 0.8747 - val_Valence_ccc: 0.3485 - val_Dominance_ccc: 0.9191\n",
      "Epoch 9/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.9169 - Arousal_loss: 0.1639 - Valence_loss: 0.6452 - Dominance_loss: 0.1077 - Arousal_ccc: 0.8361 - Valence_ccc: 0.3548 - Dominance_ccc: 0.8923 - val_loss: 0.8709 - val_Arousal_loss: 0.1211 - val_Valence_loss: 0.6721 - val_Dominance_loss: 0.0777 - val_Arousal_ccc: 0.8789 - val_Valence_ccc: 0.3279 - val_Dominance_ccc: 0.9223\n",
      "Epoch 10/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.9084 - Arousal_loss: 0.1631 - Valence_loss: 0.6380 - Dominance_loss: 0.1073 - Arousal_ccc: 0.8369 - Valence_ccc: 0.3620 - Dominance_ccc: 0.8927 - val_loss: 0.9078 - val_Arousal_loss: 0.1194 - val_Valence_loss: 0.7048 - val_Dominance_loss: 0.0836 - val_Arousal_ccc: 0.8806 - val_Valence_ccc: 0.2952 - val_Dominance_ccc: 0.9164\n",
      "Epoch 11/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.8901 - Arousal_loss: 0.1613 - Valence_loss: 0.6221 - Dominance_loss: 0.1067 - Arousal_ccc: 0.8387 - Valence_ccc: 0.3779 - Dominance_ccc: 0.8933 - val_loss: 0.8951 - val_Arousal_loss: 0.1294 - val_Valence_loss: 0.6873 - val_Dominance_loss: 0.0784 - val_Arousal_ccc: 0.8706 - val_Valence_ccc: 0.3127 - val_Dominance_ccc: 0.9216\n",
      "Epoch 12/50\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.8802 - Arousal_loss: 0.1603 - Valence_loss: 0.6145 - Dominance_loss: 0.1054 - Arousal_ccc: 0.8397 - Valence_ccc: 0.3855 - Dominance_ccc: 0.8946 - val_loss: 0.8805 - val_Arousal_loss: 0.1188 - val_Valence_loss: 0.6780 - val_Dominance_loss: 0.0837 - val_Arousal_ccc: 0.8812 - val_Valence_ccc: 0.3220 - val_Dominance_ccc: 0.9163\n"
     ]
    }
   ],
   "source": [
    "model = create_model(n_units1=256, n_units2=256)\n",
    "\n",
    "history = model.fit(x_combined, y_combined, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,                     \n",
    "                    validation_data=(x_test_scaled, y_test_dta), \n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e084594e-9150-4f33-82f7-aaf958128497",
   "metadata": {},
   "source": [
    "### Linguistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a3d2b2-55f2-47d6-ab60-d03987106dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/changhyun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f059d169-0731-412e-847e-85cbd031f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    punctuation = [i for i in ',./\\\\;:\\'@#~[{]}=+-_)(*&^%$Â£\"!`)]']\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    \n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = \"\".join([\" \" if t in punctuation else t for t in text]).lower()\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f9e2a29-c2c0-4d0e-8e7b-92b58ff7d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "transcripts_path = './transcripts/'\n",
    "sections = ['train/', 'validation/', 'test/']\n",
    "segment_path = os.path.relpath('./MSP Data/Time Labels/segments.json')\n",
    "f = open(segment_path, 'r')\n",
    "timing_data = json.load(f)\n",
    "min_time = 1.0\n",
    "\n",
    "for i, section in enumerate(sections):\n",
    "    files = fnmatch.filter(os.listdir(transcripts_path+section), '*.txt')\n",
    "    files.sort()\n",
    "    text = []\n",
    "    filename = []\n",
    "    for file in files:\n",
    "        inst = file.split('.')[0]\n",
    "        \n",
    "        if 'MSP-PODCAST_0153' in inst or 'MSP-PODCAST_1188_0020' in inst:\n",
    "            continue\n",
    "        start = timing_data[inst]['Start_Time']\n",
    "        end = timing_data[inst]['End_Time']\n",
    "        if end - start < min_time:\n",
    "            continue\n",
    "        filename.append(inst)        \n",
    "        with open(transcripts_path+section+file) as f:\n",
    "            lines = f.readlines()\n",
    "        if len(lines) == 0:\n",
    "            text.append('')\n",
    "        else:\n",
    "            text.append(lines[0])\n",
    "    if i == 0:\n",
    "        df_train = pd.DataFrame({'Filename': filename, 'txt':text})\n",
    "        \n",
    "    elif i == 1:\n",
    "        df_val = pd.DataFrame({'Filename': filename, 'txt':text})\n",
    "        \n",
    "    else:\n",
    "        df_test = pd.DataFrame({'Filename': filename, 'txt':text})\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca1710dc-1add-45ac-a991-f524417e1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned train text: 2684\n",
      "Cleaned validation text: 726\n",
      "Cleaned test text: 1498\n",
      "Cleaned all text: 4908\n"
     ]
    }
   ],
   "source": [
    "train_cleaned = df_train['txt'].apply(clean_text)\n",
    "val_cleaned = df_val['txt'].apply(clean_text)\n",
    "test_cleaned = df_test['txt'].apply(clean_text)\n",
    "combined_cleaned = pd.concat([train_cleaned, val_cleaned, test_cleaned], ignore_index=True)\n",
    "\n",
    "print(\"Cleaned train text: %d\" % (len(train_cleaned)))\n",
    "print(\"Cleaned validation text: %d\" % (len(val_cleaned)))\n",
    "print(\"Cleaned test text: %d\" % (len(test_cleaned)))\n",
    "print(\"Cleaned all text: %d\" % (len(combined_cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c560c99-3510-47e6-a374-52fed890fafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8196"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "\n",
    "for i in range(len(combined_cleaned)):\n",
    "    text = combined_cleaned[i]\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        vocab.add(token)\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "855e9673-e6a5-4c23-9db8-07053f2208c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 395\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "sentence_len = [len(sent.split()) for sent in combined_cleaned.tolist()]\n",
    "sent_len = max(sentence_len)\n",
    "print('Max sentence length: %d' % (sent_len))\n",
    "text_vec = TextVectorization(max_tokens=vocab_size, \n",
    "                             pad_to_max_tokens=True, \n",
    "                             output_sequence_length=388,\n",
    "                             output_mode='int')\n",
    "text_vec.adapt(train_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5fd0ee5-538d-43be-a517-69f6c3fe70c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "# loading pre-trained weights and build embedding layer\n",
    "embeddings_index = dict()\n",
    "embedding_dim = 300\n",
    "f = open('embeddings/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ef9067a7-5966-4141-a6db-c6ef29f7a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size+ 1, embedding_dim))\n",
    "for i, word in enumerate(text_vec.get_vocabulary()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0f6d225-03e0-46e1-9ba4-6c13e671ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size + 1, \n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=seq_len,\n",
    "                            trainable=False,\n",
    "                            name='GloVe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d179656-95c8-4667-950a-1b210af8f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate\n",
    "\n",
    "def create_bimodal_model(n_units1=64, n_units2=32):\n",
    "    ling_model = Sequential()\n",
    "    ling_inputs = Input(shape=(1,), dtype=tf.string)\n",
    "    vec = text_vec(ling_inputs)\n",
    "    embed = embedding_layer(vec)\n",
    "    lstm1 = LSTM(n_units1, return_sequences=True)(embed)\n",
    "    lstm2 = LSTM(n_units2, return_sequences=True)(lstm1)\n",
    "    lstm2 = Dropout(0.3)(lstm2)\n",
    "    ling_model = Model(inputs=ling_inputs, outputs=lstm2)\n",
    "    \n",
    "    acoustic_model = Sequential()\n",
    "    inputs = Input(shape=(seq_len, n_features), dtype=float)\n",
    "    mask = Masking()(inputs)\n",
    "    lstm_1 = LSTM(n_units1, return_sequences=True)(mask)\n",
    "    lstm_2 = LSTM(n_units2, return_sequences=True)(lstm_1)\n",
    "    lstm_2 = Dropout(0.3)(lstm_2)\n",
    "    \n",
    "    acoustic_model = Model(inputs=inputs, outputs=lstm_2)\n",
    "    \n",
    "    concat = Concatenate()([ling_model.output, acoustic_model.output])\n",
    "    lstm_last = LSTM(32, return_sequences=True)(concat)\n",
    "    output = [TimeDistributed(Dense(1), name=name)(lstm_last) for i, name in enumerate(headers)]\n",
    "    \n",
    "    bimodal_model = Model(inputs=[ling_inputs, inputs], outputs=output)\n",
    "    \n",
    "    rmsprop = RMSprop(lr=0.0001)\n",
    "    bimodal_model.compile(optimizer=rmsprop, loss=ccc_loss, metrics=[ccc])\n",
    "    return bimodal_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f21aa187-3d33-4961-8a7a-122147582cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_model = create_bimodal_model()\n",
    "\n",
    "train_val = pd.concat([train_cleaned, val_cleaned], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "808757ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "107/107 [==============================] - 29s 140ms/step - loss: 1.8093 - Arousal_loss: 0.4934 - Valence_loss: 0.8784 - Dominance_loss: 0.4375 - Arousal_ccc: 0.5066 - Valence_ccc: 0.1216 - Dominance_ccc: 0.5625 - val_loss: 1.3700 - val_Arousal_loss: 0.3345 - val_Valence_loss: 0.7469 - val_Dominance_loss: 0.2886 - val_Arousal_ccc: 0.6655 - val_Valence_ccc: 0.2531 - val_Dominance_ccc: 0.7114\n",
      "Epoch 2/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 1.4087 - Arousal_loss: 0.3282 - Valence_loss: 0.7944 - Dominance_loss: 0.2861 - Arousal_ccc: 0.6718 - Valence_ccc: 0.2056 - Dominance_ccc: 0.7139 - val_loss: 1.1782 - val_Arousal_loss: 0.2457 - val_Valence_loss: 0.7239 - val_Dominance_loss: 0.2086 - val_Arousal_ccc: 0.7543 - val_Valence_ccc: 0.2761 - val_Dominance_ccc: 0.7914\n",
      "Epoch 3/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 1.2609 - Arousal_loss: 0.2776 - Valence_loss: 0.7517 - Dominance_loss: 0.2317 - Arousal_ccc: 0.7224 - Valence_ccc: 0.2483 - Dominance_ccc: 0.7683 - val_loss: 1.0725 - val_Arousal_loss: 0.2083 - val_Valence_loss: 0.6943 - val_Dominance_loss: 0.1699 - val_Arousal_ccc: 0.7917 - val_Valence_ccc: 0.3057 - val_Dominance_ccc: 0.8301\n",
      "Epoch 4/50\n",
      "107/107 [==============================] - 11s 98ms/step - loss: 1.1656 - Arousal_loss: 0.2480 - Valence_loss: 0.7187 - Dominance_loss: 0.1989 - Arousal_ccc: 0.7520 - Valence_ccc: 0.2813 - Dominance_ccc: 0.8011 - val_loss: 1.0447 - val_Arousal_loss: 0.1773 - val_Valence_loss: 0.7286 - val_Dominance_loss: 0.1388 - val_Arousal_ccc: 0.8227 - val_Valence_ccc: 0.2714 - val_Dominance_ccc: 0.8612\n",
      "Epoch 5/50\n",
      "107/107 [==============================] - 11s 98ms/step - loss: 1.1029 - Arousal_loss: 0.2276 - Valence_loss: 0.6986 - Dominance_loss: 0.1766 - Arousal_ccc: 0.7724 - Valence_ccc: 0.3014 - Dominance_ccc: 0.8234 - val_loss: 0.9481 - val_Arousal_loss: 0.1613 - val_Valence_loss: 0.6656 - val_Dominance_loss: 0.1212 - val_Arousal_ccc: 0.8387 - val_Valence_ccc: 0.3344 - val_Dominance_ccc: 0.8788\n",
      "Epoch 6/50\n",
      "107/107 [==============================] - 11s 99ms/step - loss: 1.0535 - Arousal_loss: 0.2129 - Valence_loss: 0.6807 - Dominance_loss: 0.1599 - Arousal_ccc: 0.7871 - Valence_ccc: 0.3193 - Dominance_ccc: 0.8401 - val_loss: 0.9072 - val_Arousal_loss: 0.1527 - val_Valence_loss: 0.6446 - val_Dominance_loss: 0.1100 - val_Arousal_ccc: 0.8473 - val_Valence_ccc: 0.3554 - val_Dominance_ccc: 0.8900\n",
      "Epoch 7/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 1.0149 - Arousal_loss: 0.2027 - Valence_loss: 0.6631 - Dominance_loss: 0.1491 - Arousal_ccc: 0.7973 - Valence_ccc: 0.3369 - Dominance_ccc: 0.8509 - val_loss: 0.9052 - val_Arousal_loss: 0.1441 - val_Valence_loss: 0.6582 - val_Dominance_loss: 0.1029 - val_Arousal_ccc: 0.8559 - val_Valence_ccc: 0.3418 - val_Dominance_ccc: 0.8971\n",
      "Epoch 8/50\n",
      "107/107 [==============================] - 10s 98ms/step - loss: 0.9894 - Arousal_loss: 0.1931 - Valence_loss: 0.6569 - Dominance_loss: 0.1395 - Arousal_ccc: 0.8069 - Valence_ccc: 0.3431 - Dominance_ccc: 0.8605 - val_loss: 0.8986 - val_Arousal_loss: 0.1405 - val_Valence_loss: 0.6607 - val_Dominance_loss: 0.0974 - val_Arousal_ccc: 0.8595 - val_Valence_ccc: 0.3393 - val_Dominance_ccc: 0.9026\n",
      "Epoch 9/50\n",
      "107/107 [==============================] - 10s 98ms/step - loss: 0.9675 - Arousal_loss: 0.1885 - Valence_loss: 0.6458 - Dominance_loss: 0.1332 - Arousal_ccc: 0.8115 - Valence_ccc: 0.3542 - Dominance_ccc: 0.8668 - val_loss: 0.9048 - val_Arousal_loss: 0.1369 - val_Valence_loss: 0.6733 - val_Dominance_loss: 0.0946 - val_Arousal_ccc: 0.8631 - val_Valence_ccc: 0.3267 - val_Dominance_ccc: 0.9054\n",
      "Epoch 10/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.9471 - Arousal_loss: 0.1833 - Valence_loss: 0.6353 - Dominance_loss: 0.1285 - Arousal_ccc: 0.8167 - Valence_ccc: 0.3647 - Dominance_ccc: 0.8715 - val_loss: 0.8771 - val_Arousal_loss: 0.1333 - val_Valence_loss: 0.6540 - val_Dominance_loss: 0.0897 - val_Arousal_ccc: 0.8667 - val_Valence_ccc: 0.3460 - val_Dominance_ccc: 0.9103\n",
      "Epoch 11/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.9312 - Arousal_loss: 0.1794 - Valence_loss: 0.6274 - Dominance_loss: 0.1244 - Arousal_ccc: 0.8206 - Valence_ccc: 0.3726 - Dominance_ccc: 0.8756 - val_loss: 0.8534 - val_Arousal_loss: 0.1321 - val_Valence_loss: 0.6321 - val_Dominance_loss: 0.0893 - val_Arousal_ccc: 0.8679 - val_Valence_ccc: 0.3679 - val_Dominance_ccc: 0.9107\n",
      "Epoch 12/50\n",
      "107/107 [==============================] - 10s 96ms/step - loss: 0.9187 - Arousal_loss: 0.1773 - Valence_loss: 0.6195 - Dominance_loss: 0.1219 - Arousal_ccc: 0.8227 - Valence_ccc: 0.3805 - Dominance_ccc: 0.8781 - val_loss: 0.8751 - val_Arousal_loss: 0.1310 - val_Valence_loss: 0.6577 - val_Dominance_loss: 0.0864 - val_Arousal_ccc: 0.8690 - val_Valence_ccc: 0.3423 - val_Dominance_ccc: 0.9136\n",
      "Epoch 13/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.9079 - Arousal_loss: 0.1747 - Valence_loss: 0.6138 - Dominance_loss: 0.1194 - Arousal_ccc: 0.8253 - Valence_ccc: 0.3862 - Dominance_ccc: 0.8806 - val_loss: 0.8491 - val_Arousal_loss: 0.1302 - val_Valence_loss: 0.6335 - val_Dominance_loss: 0.0854 - val_Arousal_ccc: 0.8698 - val_Valence_ccc: 0.3665 - val_Dominance_ccc: 0.9146\n",
      "Epoch 14/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.8988 - Arousal_loss: 0.1733 - Valence_loss: 0.6073 - Dominance_loss: 0.1182 - Arousal_ccc: 0.8267 - Valence_ccc: 0.3927 - Dominance_ccc: 0.8818 - val_loss: 0.8639 - val_Arousal_loss: 0.1266 - val_Valence_loss: 0.6537 - val_Dominance_loss: 0.0837 - val_Arousal_ccc: 0.8734 - val_Valence_ccc: 0.3463 - val_Dominance_ccc: 0.9163\n",
      "Epoch 15/50\n",
      "107/107 [==============================] - 10s 98ms/step - loss: 0.8888 - Arousal_loss: 0.1708 - Valence_loss: 0.6018 - Dominance_loss: 0.1162 - Arousal_ccc: 0.8292 - Valence_ccc: 0.3982 - Dominance_ccc: 0.8838 - val_loss: 0.8433 - val_Arousal_loss: 0.1258 - val_Valence_loss: 0.6341 - val_Dominance_loss: 0.0834 - val_Arousal_ccc: 0.8742 - val_Valence_ccc: 0.3659 - val_Dominance_ccc: 0.9166\n",
      "Epoch 16/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.8790 - Arousal_loss: 0.1695 - Valence_loss: 0.5943 - Dominance_loss: 0.1152 - Arousal_ccc: 0.8305 - Valence_ccc: 0.4057 - Dominance_ccc: 0.8848 - val_loss: 0.8835 - val_Arousal_loss: 0.1269 - val_Valence_loss: 0.6725 - val_Dominance_loss: 0.0840 - val_Arousal_ccc: 0.8731 - val_Valence_ccc: 0.3275 - val_Dominance_ccc: 0.9160\n",
      "Epoch 17/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.8731 - Arousal_loss: 0.1685 - Valence_loss: 0.5903 - Dominance_loss: 0.1143 - Arousal_ccc: 0.8315 - Valence_ccc: 0.4097 - Dominance_ccc: 0.8857 - val_loss: 0.8734 - val_Arousal_loss: 0.1242 - val_Valence_loss: 0.6666 - val_Dominance_loss: 0.0826 - val_Arousal_ccc: 0.8758 - val_Valence_ccc: 0.3334 - val_Dominance_ccc: 0.9174\n",
      "Epoch 18/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.8651 - Arousal_loss: 0.1678 - Valence_loss: 0.5846 - Dominance_loss: 0.1127 - Arousal_ccc: 0.8322 - Valence_ccc: 0.4154 - Dominance_ccc: 0.8873 - val_loss: 0.8670 - val_Arousal_loss: 0.1242 - val_Valence_loss: 0.6604 - val_Dominance_loss: 0.0824 - val_Arousal_ccc: 0.8758 - val_Valence_ccc: 0.3396 - val_Dominance_ccc: 0.9176\n",
      "Epoch 19/50\n",
      "107/107 [==============================] - 10s 98ms/step - loss: 0.8594 - Arousal_loss: 0.1673 - Valence_loss: 0.5802 - Dominance_loss: 0.1120 - Arousal_ccc: 0.8327 - Valence_ccc: 0.4198 - Dominance_ccc: 0.8880 - val_loss: 0.9112 - val_Arousal_loss: 0.1237 - val_Valence_loss: 0.7048 - val_Dominance_loss: 0.0827 - val_Arousal_ccc: 0.8763 - val_Valence_ccc: 0.2952 - val_Dominance_ccc: 0.9173\n",
      "Epoch 20/50\n",
      "107/107 [==============================] - 10s 97ms/step - loss: 0.8495 - Arousal_loss: 0.1657 - Valence_loss: 0.5725 - Dominance_loss: 0.1113 - Arousal_ccc: 0.8343 - Valence_ccc: 0.4275 - Dominance_ccc: 0.8887 - val_loss: 0.8489 - val_Arousal_loss: 0.1231 - val_Valence_loss: 0.6433 - val_Dominance_loss: 0.0825 - val_Arousal_ccc: 0.8769 - val_Valence_ccc: 0.3567 - val_Dominance_ccc: 0.9175\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "history = ling_model.fit([train_val, x_combined], y_combined, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size,                     \n",
    "                    validation_data=([test_cleaned, x_test_scaled], y_test_dta), \n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6375b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " text_vectorization_2 (TextVect  (None, 388)         0           ['input_21[0][0]']               \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 388, 50)]    0           []                               \n",
      "                                                                                                  \n",
      " GloVe (Embedding)              (None, 388, 300)     2459100     ['text_vectorization_2[0][0]']   \n",
      "                                                                                                  \n",
      " masking_7 (Masking)            (None, 388, 50)      0           ['input_22[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_37 (LSTM)                 (None, 388, 64)      93440       ['GloVe[0][0]']                  \n",
      "                                                                                                  \n",
      " lstm_39 (LSTM)                 (None, 388, 64)      29440       ['masking_7[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_38 (LSTM)                 (None, 388, 32)      12416       ['lstm_37[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_40 (LSTM)                 (None, 388, 32)      12416       ['lstm_39[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 388, 32)      0           ['lstm_38[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 388, 32)      0           ['lstm_40[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 388, 64)      0           ['dropout_18[0][0]',             \n",
      "                                                                  'dropout_19[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_41 (LSTM)                 (None, 388, 32)      12416       ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " Arousal (TimeDistributed)      (None, 388, 1)       33          ['lstm_41[0][0]']                \n",
      "                                                                                                  \n",
      " Valence (TimeDistributed)      (None, 388, 1)       33          ['lstm_41[0][0]']                \n",
      "                                                                                                  \n",
      " Dominance (TimeDistributed)    (None, 388, 1)       33          ['lstm_41[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,619,327\n",
      "Trainable params: 160,227\n",
      "Non-trainable params: 2,459,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ling_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b946ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
